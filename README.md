# LLM Gateway

å¤§æ¨¡å‹ API ç½‘å…³æœåŠ¡ - å°†ç”¨æˆ·è¯·æ±‚è½¬å‘åˆ°å¤§æ¨¡å‹ API çš„ç»Ÿä¸€å…¥å£

## ç®€ä»‹

LLM Gateway æ˜¯ä¸€ä¸ªçº¯åå°æœåŠ¡ï¼Œä½œä¸ºå¤§æ¨¡å‹ API çš„ä»£ç†ç½‘å…³ã€‚å®ƒæ¥æ”¶ç”¨æˆ·ä¾§çš„ API è°ƒç”¨ï¼Œå°†è¯·æ±‚è½¬æ¢ä¸ºå¤§æ¨¡å‹ API çš„æ ‡å‡†æ ¼å¼ï¼Œç„¶åå°†å¤§æ¨¡å‹çš„å“åº”è¿”å›ç»™ç”¨æˆ·ä¾§ã€‚

## æ ¸å¿ƒåŠŸèƒ½

- ğŸ”„ **è¯·æ±‚è½¬å‘**ï¼šæ¥æ”¶ç”¨æˆ·è¯·æ±‚å¹¶è½¬å‘åˆ°å¤§æ¨¡å‹ API
- ğŸ”€ **è¯·æ±‚è½¬æ¢**ï¼šå°†ç”¨æˆ·è¯·æ±‚æ ¼å¼è½¬æ¢ä¸ºå¤§æ¨¡å‹ API æ ‡å‡†æ ¼å¼
- ğŸ›¡ï¸ **é”™è¯¯å¤„ç†**ï¼šç»Ÿä¸€çš„é”™è¯¯å¤„ç†å’Œå“åº”æ ¼å¼
- ğŸ“ **æ—¥å¿—è®°å½•**ï¼šå®Œæ•´çš„è¯·æ±‚å’Œå“åº”æ—¥å¿—
- âš™ï¸ **é…ç½®ç®¡ç†**ï¼šçµæ´»çš„ç¯å¢ƒå˜é‡é…ç½®

## æŠ€æœ¯æ ˆ

- **æ¡†æ¶**: Express.js
- **è¯­è¨€**: TypeScript
- **LLM SDK**: OpenAI SDK (å®˜æ–¹ SDK)
- **è¿è¡Œæ—¶**: Node.js 18+

## é¡¹ç›®æ¶æ„

```
llm-gateway/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ server.ts              # æœåŠ¡å™¨å…¥å£æ–‡ä»¶
â”‚   â”œâ”€â”€ routes/                # è·¯ç”±å±‚
â”‚   â”‚   â””â”€â”€ chat.ts            # èŠå¤© API è·¯ç”±
â”‚   â”œâ”€â”€ services/              # ä¸šåŠ¡æœåŠ¡å±‚
â”‚   â”‚   â””â”€â”€ llmClient.ts       # LLM API å®¢æˆ·ç«¯æœåŠ¡
â”‚   â”œâ”€â”€ types/                 # TypeScript ç±»å‹å®šä¹‰
â”‚   â”‚   â””â”€â”€ index.ts
â”‚   â”œâ”€â”€ utils/                 # å·¥å…·å‡½æ•°
â”‚   â”‚   â”œâ”€â”€ logger.ts          # æ—¥å¿—å·¥å…·
â”‚   â”‚   â””â”€â”€ errorHandler.ts    # é”™è¯¯å¤„ç†
â”‚   â””â”€â”€ config/                # é…ç½®ç®¡ç†
â”‚       â””â”€â”€ index.ts
â”œâ”€â”€ dist/                      # ç¼–è¯‘è¾“å‡ºç›®å½•
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .eslintrc.json
â”œâ”€â”€ env.example                # ç¯å¢ƒå˜é‡ç¤ºä¾‹
â”œâ”€â”€ package.json
â”œâ”€â”€ tsconfig.json
â””â”€â”€ README.md
```

## æ¶æ„è¯´æ˜

### 1. æœåŠ¡å™¨å…¥å£ (`src/server.ts`)
- Express æœåŠ¡å™¨åˆå§‹åŒ–
- ä¸­é—´ä»¶é…ç½®ï¼ˆCORSã€JSON è§£æã€æ—¥å¿—ï¼‰
- è·¯ç”±æ³¨å†Œ
- é”™è¯¯å¤„ç†
- ä¼˜é›…å…³é—­

### 2. è·¯ç”±å±‚ (`src/routes/chat.ts`)
- æ¥æ”¶ç”¨æˆ· HTTP è¯·æ±‚
- éªŒè¯è¯·æ±‚æ ¼å¼
- è°ƒç”¨ LLM æœåŠ¡
- è¿”å›å“åº”

### 3. æœåŠ¡å±‚ (`src/services/llmClient.ts`)
- å°è£…å¤§æ¨¡å‹ API è°ƒç”¨é€»è¾‘
- è¯·æ±‚æ ¼å¼è½¬æ¢
- HTTP è¯·æ±‚å¤„ç†
- å“åº”æ•°æ®è§£æ

### 4. é…ç½®å±‚ (`src/config/index.ts`)
- ç»Ÿä¸€ç®¡ç†ç¯å¢ƒå˜é‡
- é…ç½®éªŒè¯
- é»˜è®¤å€¼è®¾ç½®

### 5. å·¥å…·å±‚ (`src/utils/`)
- æ—¥å¿—è®°å½•å·¥å…·
- é”™è¯¯å¤„ç†å·¥å…·
- é€šç”¨å·¥å…·å‡½æ•°

## å¼€å§‹ä½¿ç”¨

### 1. å®‰è£…ä¾èµ–

```bash
npm install
# æˆ–
yarn install
# æˆ–
pnpm install
```

### 2. é…ç½®ç¯å¢ƒå˜é‡

å¤åˆ¶ `env.example` æ–‡ä»¶ä¸º `.env`ï¼š

```bash
cp env.example .env
```

ç¼–è¾‘ `.env` æ–‡ä»¶ï¼Œè®¾ç½®ä»¥ä¸‹å˜é‡ï¼š

```env
# LLM API é…ç½®
LLM_API_BASE_URL=https://api.openai.com/v1
LLM_API_KEY=your-api-key-here

# æœåŠ¡é…ç½®
PORT=3000
NODE_ENV=development
```

### 3. å¼€å‘

```bash
npm run dev
```

æœåŠ¡å°†åœ¨ `http://localhost:3000` å¯åŠ¨

### 4. æ„å»ºç”Ÿäº§ç‰ˆæœ¬

```bash
npm run build
npm start
```

## API ä½¿ç”¨è¯´æ˜

### POST /api/chat

å‘é€èŠå¤©è¯·æ±‚åˆ°å¤§æ¨¡å‹ APIã€‚

**è¯·æ±‚ç¤ºä¾‹ï¼š**

```bash
curl -X POST http://localhost:3000/api/chat \
  -H "Content-Type: application/json" \
  -d '{
    "messages": [
      {
        "role": "user",
        "content": "ä½ å¥½ï¼Œè¯·ä»‹ç»ä¸€ä¸‹ä½ è‡ªå·±"
      }
    ],
    "model": "gpt-3.5-turbo",
    "temperature": 0.7,
    "max_tokens": 1000
  }'
```

**è¯·æ±‚å‚æ•°ï¼š**

- `messages` (å¿…éœ€): æ¶ˆæ¯æ•°ç»„ï¼Œæ¯ä¸ªæ¶ˆæ¯åŒ…å« `role` å’Œ `content`
  - `role`: "system" | "user" | "assistant"
  - `content`: æ¶ˆæ¯å†…å®¹
- `model` (å¯é€‰): æ¨¡å‹åç§°ï¼Œé»˜è®¤ä½¿ç”¨é…ç½®ä¸­çš„é»˜è®¤æ¨¡å‹ã€‚å¦‚æœå¯ç”¨äº†æ¨¡å‹éªŒè¯ï¼Œå¿…é¡»ä½¿ç”¨æ”¯æŒçš„æ¨¡å‹
- `temperature` (å¯é€‰): æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶éšæœºæ€§ï¼ˆé»˜è®¤ï¼š0.7ï¼‰
- `max_tokens` (å¯é€‰): æœ€å¤§ token æ•°ï¼ˆé»˜è®¤ï¼š2000ï¼‰
- `stream` (å¯é€‰): æ˜¯å¦ä½¿ç”¨æµå¼å“åº”

**å“åº”ç¤ºä¾‹ï¼š**

```json
{
  "id": "chatcmpl-xxx",
  "object": "chat.completion",
  "created": 1234567890,
  "model": "gpt-3.5-turbo",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "ä½ å¥½ï¼æˆ‘æ˜¯..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 10,
    "completion_tokens": 20,
    "total_tokens": 30
  }
}
```

### GET /api/chat

å¥åº·æ£€æŸ¥ç«¯ç‚¹ï¼Œç”¨äºæ£€æŸ¥æœåŠ¡æ˜¯å¦æ­£å¸¸è¿è¡Œã€‚

**å“åº”ç¤ºä¾‹ï¼š**

```json
{
  "status": "ok",
  "message": "LLM Gateway API è¿è¡Œæ­£å¸¸",
  "timestamp": "2024-01-01T00:00:00.000Z"
}
```

### GET /api/models

è·å–æ”¯æŒçš„æ¨¡å‹åˆ—è¡¨å’Œé…ç½®ä¿¡æ¯ã€‚

**å“åº”ç¤ºä¾‹ï¼š**

```json
{
  "defaultModel": "gpt-3.5-turbo",
  "supportedModels": [
    "gpt-4",
    "gpt-4-turbo",
    "gpt-4-turbo-preview",
    "gpt-4-0125-preview",
    "gpt-4-1106-preview",
    "gpt-3.5-turbo",
    "gpt-3.5-turbo-16k",
    "gpt-3.5-turbo-0125",
    "gpt-3.5-turbo-1106"
  ],
  "defaultParams": {
    "temperature": 0.7,
    "maxTokens": 2000,
    "topP": 1.0,
    "frequencyPenalty": 0.0,
    "presencePenalty": 0.0
  },
  "modelValidationEnabled": true
}
```

### GET /

æ ¹è·¯å¾„å¥åº·æ£€æŸ¥ã€‚

**å“åº”ç¤ºä¾‹ï¼š**

```json
{
  "status": "ok",
  "message": "LLM Gateway API æœåŠ¡è¿è¡Œæ­£å¸¸",
  "timestamp": "2024-01-01T00:00:00.000Z",
  "version": "0.1.0"
}
```

## é”™è¯¯å¤„ç†

æœåŠ¡ä½¿ç”¨ç»Ÿä¸€çš„é”™è¯¯å“åº”æ ¼å¼ï¼š

```json
{
  "error": {
    "message": "é”™è¯¯æè¿°",
    "type": "é”™è¯¯ç±»å‹",
    "code": "é”™è¯¯ä»£ç "
  }
}
```

å¸¸è§é”™è¯¯ç ï¼š
- `400`: è¯·æ±‚å‚æ•°é”™è¯¯
- `401`: è®¤è¯å¤±è´¥
- `404`: æ¥å£ä¸å­˜åœ¨
- `500`: æœåŠ¡å™¨å†…éƒ¨é”™è¯¯

## å¼€å‘è„šæœ¬

- `npm run dev` - å¯åŠ¨å¼€å‘æœåŠ¡å™¨ï¼ˆä½¿ç”¨ tsx watch è‡ªåŠ¨é‡å¯ï¼‰
- `npm run build` - æ„å»ºç”Ÿäº§ç‰ˆæœ¬ï¼ˆç¼–è¯‘ TypeScriptï¼‰
- `npm start` - å¯åŠ¨ç”Ÿäº§æœåŠ¡å™¨
- `npm run lint` - è¿è¡Œ ESLint æ£€æŸ¥
- `npm run type-check` - è¿è¡Œ TypeScript ç±»å‹æ£€æŸ¥

## è®¸å¯è¯

MIT
